{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f827ba4f-2a68-4019-89d7-f96241eefe7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Wikipedia Analysis\n",
    "\n",
    "# **Final Practical Work**\n",
    "\n",
    "# Alejo González García (100454351)\n",
    "# Andrés Navarro Pedregal (100451730)\n",
    "\n",
    "\n",
    "This notebook IS NOT an skeleton. Is a sample of instruccions to analyse in classroom the Wikipedia Dataset provided by Databricks\n",
    "\n",
    "During the class we will fill cells to implement the PageRank algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000ae955-cd3b-4a52-9e0c-978241c38d5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10865609-2787-4474-900b-54e4dd830765",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import ArrayType, StringType,LongType\n",
    "from pyspark.sql.functions import lower, size, explode, collect_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae6143a-d173-4019-861b-e9955817c173",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29455663-e769-4c76-a3b0-370d20b553a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wikipediaDF=spark.read.parquet(\"dbfs:/databricks-datasets/wikipedia-datasets/data-001/en_wikipedia/articles-only-parquet\")\n",
    "# As we have seen in class and in the document, the database we are using is the Wikipedia datasets with all the available links that this program, Databricks has already collected in this link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16e48be-33f7-47c7-8265-a94866829c01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# wikipediaDF.count() # We already know that there are 5823210 entries, so we just run this command once to check as it takes a lot of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f1bd9c8-7f2f-4780-b338-8e55fbd947a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PartialWikipediaDF=wikipediaDF.sample(fraction=0.001,seed=0).cache()\n",
    "# We have been suggested to use no more than the 0.01% of the data, but we will use the 0.1% (fraction=0.001) for this last experiment to have better results as it just takes about 5 minutes. \n",
    "# Notice that we have to store the data frame in the cache memory so that it can be accessed directly during the code execution without wasting resources multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a14781c-d035-43fc-bf5b-b5a21992fc51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PartialWikipediaDF.count() \n",
    "# We are skipping this command as it takes a lot of time. We have just run it once, and as requested it counts 576 samples, that is the 0.01% of the full data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(PartialWikipediaDF) \n",
    "# Here we can have an insight of the data with the expected variables (id, revisionId, username, id, text...)\n",
    "# We are also skipping this command to save resources each type we execute the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611c112a-a5b2-4213-9715-edfa9cc4f875",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# We have been given this function that takes a document and returns all the links inside that document: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77276afc-1f7c-4a23-860c-c923682a882e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We have decided to change it a bit the provided function and we have gained some improvements: \n",
    "def parse_links(document_body):       \n",
    "    titles = re.findall(r'\\[\\[(.+?)\\]\\]',document_body.lower())\n",
    "    if len(titles) == 0:\n",
    "        return []\n",
    "    return list(set([title.lower() for title in titles])) # to remove duplicates and make all lowercase\n",
    "# We are retrieving a full list with all the links inside!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91652eea-ca38-4c8d-9291-7ad0f60fd66f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here we are applying the UDF (User Defined Function that was requested in the statement)\n",
    "parse_links_udf = udf(parse_links,ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ae3725-3c11-4190-b0e0-e220827c6361",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_forward_df = PartialWikipediaDF.select(\"title\",\"id\",parse_links_udf(\"text\").alias(\"links\"))\n",
    "# This links column represents the list of titles that contains that specificic id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac688e4-ffcc-413d-b2c2-d7e5d9091ded",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(temp_forward_df) # If we uncomment this line we can see the temporal forward dataframe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40372e97-1162-4d60-ac36-a5b258644dc2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# On class, we have defined this function, Titles2ID, that is going to map the titles with the IDs of the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b99d7646-6d8e-4a4b-807a-7916b25391d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We have made some adjustments:\n",
    "# The links is a list of titles with the output documents\n",
    "# The data titles is the DAta Frame that maps the titles and the ids. \n",
    "def Titles2ID (links, data_titles):\n",
    "    if len(links) == 0:\n",
    "        return []\n",
    "    return list(set(data_titles[data_titles.title.isin(links)].id.to_list())) # to remove duplicates\n",
    "# This function is returning for the provided titles, a list full of the respective IDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35a0886-01c8-465f-a0bf-339539cbbf4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If titles is empty it returns an empty list, otherwise, it returs the id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b6981f0-ae9c-4bf3-a2b1-83bbcda1ad01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Titles_ID_PDF = temp_forward_df.select(lower(\"title\").alias(\"title\"),\"id\").toPandas() # This data frame contains ONLY the id and title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33e0eae5-5082-442b-9d6e-500146d9ff2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Titles2ID_udf = udf(lambda x: Titles2ID(x, Titles_ID_PDF), ArrayType(LongType(),False)) # We create again a UDF (User Defined Function) so that we can get the spark versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6530d4-724b-483a-89e9-59951e959ae0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forward_df = temp_forward_df.select(\"id\", \"title\", Titles2ID_udf(\"links\").alias(\"links\")).cache()\n",
    "# Here we are placing on the links column the output titles of the provided page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e626b9-37dc-497c-97ea-d9944cf30351",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outgoings_links_counters_pdf = forward_df.select(\"id\", \"title\", \"links\", size(\"links\").alias(\"count_output_links\")).toPandas()\n",
    "# Here we are defining the counter of outgoing links so that we obtain the IDs of the input pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08eb7cc5-cc7e-4bb8-8d62-30ccaedd2565",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# This function, input_link is going to retrieve the links of a document with the use of the ouput of the provided document: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e9b57b3-b9cb-4ba2-a149-0e2981754f59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# document id is defined by it´s name, and links are the tuple of output IDS\n",
    "def input_link(document_id, links):\n",
    "    if len(links) == 0:\n",
    "        return []\n",
    "    \n",
    "    return [(link, document_id) for link in links]\n",
    "# As we say in the title, this retrieves the links of a document in a reversed order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584f150c-c4cc-45c0-9e7b-2daae4c8f0fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Now we are going to reverse the ouput and obtain the input links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff12df4-a176-4a2f-8267-35161b55ad22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reverse_rdd=(forward_df.rdd\n",
    "            .flatMap(lambda r: input_link(r.id, r.links)) # We just need these two columns of the original DF\n",
    "            .groupByKey()\n",
    "            .map(lambda r: (r[0], list(r[1]), [int(outgoings_links_counters_pdf.loc[outgoings_links_counters_pdf['id'] == s, 'count_output_links'].values[0]) for s in list(r[1])] )) \n",
    "            )\n",
    "\n",
    "reverse_df = spark.createDataFrame(reverse_rdd,[\"id\", \"links\", \"outgoings_links_counters_pdf\"]) \n",
    "# reverse_df stores the in/out links of each of the documents that we have. And we can also see the amount of output links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "645ce30f-532d-419e-83bf-4c20c920c9be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We have to handle the fact that users stop searching. In practice, the Page Rank algorithm adds a damping factor at each stage to model it, \n",
    "# here we take into account the case in which a page has NO input links. \n",
    "reverse_pdf = reverse_df.toPandas()\n",
    "\n",
    "for i in forward_df.toPandas()['id']:\n",
    "    if i not in reverse_pdf['id'].values:\n",
    "        reverse_pdf.loc[reverse_pdf.shape[0]]= [i, [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcd384f2-a106-40f6-810b-7029863146cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "N = len(reverse_pdf)\n",
    "reverse_pdf['PageRank'] = float(0.85 / N) # Notice that 0.85 is the damping factor!\n",
    "d = sc.broadcast(0.85) \n",
    "broadcast_count_total = sc.broadcast(N)\n",
    "broadcast_count_links_pdf = sc.broadcast(outgoings_links_counters_pdf) \n",
    "#The broadcast term allows that variable to be used by any entity in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d0a22e-1539-46eb-89f8-5b771e42fd19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# This convergence function is a binary one, that returns 1 if the difference does not exceed the threshold and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7dba625-1506-483f-9288-44dcab6aaefc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convergence(document_id, prev_page_rank, new_page_rank, threshold):    \n",
    "    previous_value = prev_page_rank.loc[prev_page_rank['id'] == document_id, 'PageRank'].values[0]\n",
    "    new_value = new_page_rank.loc[new_page_rank['id'] == document_id, 'PageRank'].values[0]\n",
    "    \n",
    "    return 1 if (previous_value - new_value) < threshold else 0\n",
    "#The function has as inputes the id of the document, the previous and new page rank of that page and the stablished threshold. With all of this, \n",
    "# it returns a 1 or a 0 if the difference between the previous value and the new one is lower than the threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b83114c-7d03-4a59-8675-48f05c1a4caf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FullReversed = sqlContext.createDataFrame(reverse_pdf)\n",
    "FullReversedPDF = FullReversed.toPandas()\n",
    "\n",
    "iterations = 20 # Maximum amount of times this loop can be run. \n",
    "threshold = 0.00001 # Convergence threshold\n",
    "new_page_rank_df = FullReversed.select(\"id\", 'links', 'outgoings_links_counters_pdf', \"PageRank\")\n",
    "N = broadcast_count_total.value # This is the number of total pages. \n",
    "num_links = broadcast_count_links_pdf.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e4fbe8-bc24-4312-a7a7-8e14d0e7c53a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Page Rank Algorithm Implementation\n",
    "In the following for loop, we are going to compute the final Page Rank. If we number of iterations, 20, is reached, the loop will end and we have finished. And if Page Rank has converged, we will also break. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a006e9ae-1c33-4785-a67a-4000271de696",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    share = 0\n",
    "    \n",
    "    for index in range(len(FullReversedPDF)):\n",
    "        id_link = FullReversedPDF.loc[index, 'id']\n",
    "\n",
    "        if (num_links.loc[num_links['id'] == id_link, 'count_output_links'].values[0]) == 0: \n",
    "            # This condition is reached when the node is floating, no links associated to it, and we have to redistribute the rank: \n",
    "            page_rank = FullReversedPDF.loc[index, 'PageRank']\n",
    "            share = page_rank / N \n",
    "        # Following the algorithm, we must compute the share, that it´s as we have been explained, the page rank of that web over N, the number of total pages. \n",
    "    \n",
    "    new_page_rank_pdf = new_page_rank_df.toPandas() # creating the pandas df\n",
    "    \n",
    "    for index in range(len(FullReversedPDF)): \n",
    "        # Again, following the algorithm steps, at each iteration the rank for ALL the documents must be updated\n",
    "        temp_num_links = FullReversedPDF.loc[index, 'outgoings_links_counters_pdf']\n",
    "        list_of_ids = FullReversedPDF.loc[index, 'links']\n",
    "        \n",
    "        # Notice that obviously we have to define a new rank \n",
    "        new_rank = share\n",
    "        if len(list_of_ids) != 0:\n",
    "            # When there is at least one id, we proceed: \n",
    "            for l in range(len(list_of_ids)):\n",
    "                new_rank += ((new_page_rank_pdf.loc[new_page_rank_pdf['id'] == list_of_ids[l], 'PageRank'].values[0]) / temp_num_links[l])\n",
    "                # Here we are just applying the general formulation to get the new rank of a page. \n",
    "\n",
    "        new_page_rank_pdf.loc[index, 'PageRank'] = float( ((1 - d.value) / N) + (d.value * new_rank) )\n",
    "        # As explained in step 9 of the document, we must apply a damping factor so that at each stage we model the fact that users stop searching\n",
    "\n",
    "    new_page_rank_df = sqlContext.createDataFrame(new_page_rank_pdf)\n",
    "    \n",
    "    #Now we are going to apply the requested UDF (User Defined Function), which parses the text field from each record, and extracts the outgoing links.\n",
    "    # It will check for convergence\n",
    "    UDF_checking =  udf(lambda l: convergence(l, FullReversedPDF, new_page_rank_pdf, threshold), FloatType())\n",
    "    ConvergenceCheckDataFrame = FullReversed.select(\"id\", UDF_checking(\"id\").alias(\"Condition\")).toPandas()\n",
    "    # Here we have just computed the convergence and now we are going to check if it satisties the condition: \n",
    "    \n",
    "    FullReversedPDF = new_page_rank_pdf\n",
    "                                                                         \n",
    "    if ConvergenceCheckDataFrame['Condition'].sum() == N: \n",
    "        break\n",
    "    # If we dont break means that the Page Rank is not computed yet, that we have not reached the N maximum. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c30e984-29f2-4bb1-98b8-1ff5fe6830cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_page_rank_df = sqlContext.createDataFrame(new_page_rank_pdf) # Creating the Data Fram of the Newest Page Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023426ed-97e8-4701-85ff-1f1fae71142f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Now we are going to check for Correctness of our results: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c090fa2-bdde-4b32-b156-3c687cd1b640",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|PageRank             |\n",
      "+---------------------+\n",
      "|4.742657426317904E-5 |\n",
      "|3.1418983320806394E-4|\n",
      "|3.6531280175691966E-5|\n",
      "|5.492433516006009E-4 |\n",
      "|1.3736072465105387E-4|\n",
      "|4.0671567608422055E-4|\n",
      "|1.7086809609378057E-4|\n",
      "|1.7086130588891258E-4|\n",
      "|1.6991290964813455E-4|\n",
      "|1.058048828092645E-4 |\n",
      "+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Out[26]: 14"
     ]
    }
   ],
   "source": [
    "new_page_rank_df.select('PageRank').distinct().show(10, False) # showing first 10 results: \n",
    "len(new_page_rank_pdf['PageRank'].unique()) # Checking the length of the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22196dfd-cce4-4815-8cc0-dc69825218ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------------------------+--------------------+\n",
      "|      id|links|outgoings_links_counters_pdf|            PageRank|\n",
      "+--------+-----+----------------------------+--------------------+\n",
      "|42043097|   []|                          []|2.563598608820488...|\n",
      "|   55578|   []|                          []|2.563598608820488...|\n",
      "|39317197|   []|                          []|2.563598608820488...|\n",
      "|  193013|   []|                          []|2.563598608820488...|\n",
      "|10598848|   []|                          []|2.563598608820488...|\n",
      "|   60303|   []|                          []|2.563598608820488...|\n",
      "|19435596|   []|                          []|2.563598608820488...|\n",
      "|   14800|   []|                          []|2.563598608820488...|\n",
      "| 3356661|   []|                          []|2.563598608820488...|\n",
      "|   68554|   []|                          []|2.563598608820488...|\n",
      "|19437501|   []|                          []|2.563598608820488...|\n",
      "|   23343|   []|                          []|2.563598608820488...|\n",
      "|11133477|   []|                          []|2.563598608820488...|\n",
      "|   68882|   []|                          []|2.563598608820488...|\n",
      "|19485841|   []|                          []|2.563598608820488...|\n",
      "+--------+-----+----------------------------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_page_rank_df.orderBy(\"PageRank\").show(15) # Showing 15 samples to check the correctness: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be0d04e0-3fd6-426d-b6f7-5f933d60d739",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Alejo´s Individual Conclusions: \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3928583621355436,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "100454351-100451730-PageRankWikipedia",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
