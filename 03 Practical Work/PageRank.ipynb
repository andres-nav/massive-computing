{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2828b0bf-b96a-42a2-be06-5c4e0212e5fd",
     "showTitle": false,
     "title": ""
    },
    "id": "aTVYBaYX5eOo"
   },
   "source": [
    "# **Final Practical Work**\n",
    "\n",
    "### **Code developers:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76344443-4013-46b7-8ccb-52ce8b9c854f",
     "showTitle": false,
     "title": ""
    },
    "id": "CMUs_bg76B16"
   },
   "source": [
    "This final project tries to obtain the page rank values of the Wikipedia pages. Pyspark, the Python API for Apache Spark is used, so that massive amounts of data, like the millions of wikipedia pages, can be included in the process. In particular, this assignment has been carried out in Databricks, which is a platform for working with spark, that connects with a remote cluster that handles and stores the massive amounts of data being processed, mainly in Dataframe formats. As we will see in the following sections, in order to obtain the PageRank of each Wikipedia document, a set of steps need to be followed. Firstly, the data is preprocessed and cleaned. Then, several Dataframes containing different information are created. Finally, the page rank function is coded and the results are checked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2bf31aa-5b4f-4463-8a00-3e4d016fe0a8",
     "showTitle": false,
     "title": ""
    },
    "id": "6rHlWnv65dIR"
   },
   "source": [
    "## **1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae31bda-c6bb-4bfd-a02c-acce7def255b",
     "showTitle": false,
     "title": ""
    },
    "id": "iefhUAZx5dIT"
   },
   "outputs": [],
   "source": [
    "# We first need to import the packages and functions that are characteristic from spark\n",
    "# and that are used for data manipulation\n",
    "\n",
    "import pandas as pd # library that deals with data (not in spark format)\n",
    "import re # library used for regular expressions (will be used for extracting the links in the documents)\n",
    "\n",
    "# We now proceed to import all the pyspark libraries\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import ArrayType, StringType,LongType\n",
    "from pyspark.sql.functions import size, explode, collect_list \n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8fb879b-5825-456f-b79f-dd08937ecfe5",
     "showTitle": false,
     "title": ""
    },
    "id": "d8VZckfN5dIV"
   },
   "source": [
    "## **2. Reading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9169f4f1-fc21-46a6-b37f-e7dab2a9147e",
     "showTitle": false,
     "title": ""
    },
    "id": "_P0AhF3e5dIV"
   },
   "outputs": [],
   "source": [
    "# The dataset that we will use in this project is the wikipedia documents that can be found in the web\n",
    "# Databricks has a pickle file with the latter information \n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "wikipediaDF = spark.read.parquet(\"dbfs:/databricks-datasets/wikipedia-datasets/data-001/en_wikipedia/articles-only-parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cc9c86a-e8d1-464f-8e61-93b5973a76ec",
     "showTitle": false,
     "title": ""
    },
    "id": "hSrIabRl5dIW"
   },
   "outputs": [],
   "source": [
    "# Select a random subset of the data for efficiency purposes and save it in cache\n",
    "# When we save in cache a DF, eveytime it is used in the code, it does not have\n",
    "# to be computed again, but instead it is accessible in the cache memory\n",
    "PartialWikipediaDF = wikipediaDF.sample(fraction=0.01,seed=0).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76bb3036-dc45-4a27-a8d1-fe371c69dc50",
     "showTitle": false,
     "title": ""
    },
    "id": "8vroTgvc5dIX"
   },
   "source": [
    "## **3. Extracting the links in a doc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f82a8fea-fa50-4d80-9858-5f8a3b0affb1",
     "showTitle": false,
     "title": ""
    },
    "id": "OOQYVY995dIX"
   },
   "outputs": [],
   "source": [
    "# This function receives a string (wikipedia text), and outputs a list of strings containing\n",
    "# the titles of the documents it points to (output links)\n",
    "\n",
    "def parse_titles(document_body):\n",
    "    \"\"\"\n",
    "    Input: text of a wikipedia document\n",
    "    Output: list of strings containing the titles of the documents it points to\n",
    "    \n",
    "    Regular expressions are used in order to extract the information within the text\n",
    "    that is inside \"[[]]\". For example \"[[HELLO]]\" will capture \"HELLO\".\n",
    "    \"\"\"\n",
    "        \n",
    "    titles = re.findall(r'\\[\\[(.+?)\\]\\]',document_body.lower())\n",
    "\n",
    "    if len(titles) == 0:\n",
    "        return []\n",
    "\n",
    "    return list(set([title.lower() for title in titles])) # to remove duplicates and make all lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c075bfaf-7a5b-425c-8c7e-c5b10721115d",
     "showTitle": false,
     "title": ""
    },
    "id": "fZ0Hsq5y5dIX"
   },
   "outputs": [],
   "source": [
    "# In order to use the parse_links function with spark DFs, we need to create a user defined function\n",
    "parse_titles_udf = udf(parse_titles, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a7a46b5-2d5c-4f4e-9ae4-45f875784c98",
     "showTitle": false,
     "title": ""
    },
    "id": "mx7DjQxS5dIY"
   },
   "outputs": [],
   "source": [
    "# We create a new column in the DF, \"links\" that contain a list with the titles of the \n",
    "# documents that are pointed to, from the respective id Wikipedia page\n",
    "TempForwardDF = PartialWikipediaDF.select(\"title\", \"id\", parse_titles_udf(\"text\").alias(\"links\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6037d71-1fc2-46b4-88b3-b2c11fee0027",
     "showTitle": false,
     "title": ""
    },
    "id": "xmfmhC165dIY"
   },
   "source": [
    "## **4. Mapping titles to IDs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2965186d-4616-4448-9179-5faf5734d424",
     "showTitle": false,
     "title": ""
    },
    "id": "JvRa8GfG5dIZ"
   },
   "outputs": [],
   "source": [
    "# This function extracts the IDs corresponding to the list of titles extracted in \n",
    "# parse_links\n",
    "\n",
    "def titles_to_id(links, data_titles):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        - Links: list with titles of output documents\n",
    "        - titleidPDF: pandas DF containing the mapping between titles and IDs of wikipedia documents\n",
    "    Output:\n",
    "        - List of IDs of the input titles \n",
    "    \n",
    "    \"\"\"\n",
    "    if len(links) == 0:\n",
    "        return []\n",
    "    \n",
    "    return list(set(data_titles[data_titles.title.isin(links)].id.to_list())) # to remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c813bb16-7b4c-464f-abbf-ba310c17cf76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a DF with only id and title information\n",
    "\n",
    "title_id_PDF = TempForwardDF.select(lower(\"title\").alias(\"title\"),\"id\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd85dbb-e9b3-4702-baac-f5ceb85cf6cb",
     "showTitle": false,
     "title": ""
    },
    "id": "2DsOX-Ru5dIZ"
   },
   "outputs": [],
   "source": [
    "# In order to use the titles2id function with spark DFs, we need to create a user defined function\n",
    "titles_to_id_UDF = udf(lambda x: titles_to_id(x, title_id_PDF), ArrayType(LongType(),False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412e9ece-1517-4eab-9bf9-b7d37b5a6615",
     "showTitle": false,
     "title": ""
    },
    "id": "op0W0oMD5dIZ"
   },
   "outputs": [],
   "source": [
    "# We apply the previous function to the output titles of a doc, to obtain their IDs\n",
    "# Those will be stored in the \"links\" column\n",
    "ForwardDF = TempForwardDF.select(\"id\", \"title\", titles_to_id_UDF(\"links\").alias(\"links\")).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53018263-c18e-4eac-887f-3c0f415d275d",
     "showTitle": false,
     "title": ""
    },
    "id": "z5rE7Hf55dIa"
   },
   "source": [
    "## **5. Counting number of output links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c86dc47-3ea8-4d63-be86-45cef83a83ac",
     "showTitle": false,
     "title": ""
    },
    "id": "pkZLe4D-5dIa"
   },
   "outputs": [],
   "source": [
    "# We will now create a new variable, count_output_links, computed as the length of the list of \"links\" column\n",
    "OutgoingsLinksCountersDF = ForwardDF.select(\"id\", \"title\", \"links\", size(\"links\").alias(\"count_output_links\"))\n",
    "# The most efficient way of obtaining the counter of links is by using the pyspark function size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b2183e-20f5-4a9a-bf68-758fb81cb2ec",
     "showTitle": false,
     "title": ""
    },
    "id": "T1P0d8Ge5dIa"
   },
   "outputs": [],
   "source": [
    "OutgoingsLinksCountersPDF = OutgoingsLinksCountersDF.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57469070-8f2d-44b9-8106-d8a33451f5b1",
     "showTitle": false,
     "title": ""
    },
    "id": "66VY39hY5dIa"
   },
   "source": [
    "## **6. Constructing reverse table with input links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1958e1e9-6760-44df-a563-e3628aacbe2d",
     "showTitle": false,
     "title": ""
    },
    "id": "eFGkDhBI5dIa"
   },
   "outputs": [],
   "source": [
    "# This function aims to extract the input links of a document\n",
    "# by using the output links information. We use the information that \n",
    "# link x is an output of document y to extract that y is and input to x\n",
    "\n",
    "def input_link(document_id, links):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        - id: id of the document\n",
    "        - links: list of output IDs\n",
    "    Output:\n",
    "        - Reversed list containing output id - original id\n",
    "    \"\"\"\n",
    "    if len(links) == 0:\n",
    "        return []\n",
    "    \n",
    "    return [(link, document_id) for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddfb52f5-cb1d-49b1-b0bf-55930a6f34e2",
     "showTitle": false,
     "title": ""
    },
    "id": "B5IkKhJi5dIb"
   },
   "outputs": [],
   "source": [
    "# We will now apply the previous function to the DF that contains the output links\n",
    "# and we will extract the input links\n",
    "\n",
    "ForwardRDD = ForwardDF.rdd\n",
    "\n",
    "ReverseRDD=(ForwardRDD\n",
    "            .flatMap(lambda r: input_link(r.id, r.links)) # We just need these two columns of the original DF\n",
    "            .groupByKey()\n",
    "            .map(lambda r: (r[0], list(r[1]), [int(OutgoingsLinksCountersPDF.loc[OutgoingsLinksCountersPDF['id']==s, 'count_output_links'].values[0]) for s in list(r[1])] )) \n",
    "            )\n",
    "\n",
    "reverseDF = spark.createDataFrame(ReverseRDD,[\"id\", \"links\", \"OutgoingsLinksCountersPDF\"]) \n",
    "# This new Dataframe will contain the input links and \n",
    "# output links of each of the Wikipedia documents. In addition, \n",
    "# the variable OutgoingsLinksCountersPDF shows the total amount of output links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49498808-ffb5-445a-809d-1de4d7dc48c0",
     "showTitle": false,
     "title": ""
    },
    "id": "a9Cdw_KO5dIb"
   },
   "outputs": [],
   "source": [
    "# case when id not in reverseDF because it does not have incoming links\n",
    "# The id is added to the reverseDF, but the rest of the columns are empty\n",
    "reversePDF = reverseDF.toPandas()\n",
    "forwardPDF = ForwardDF.toPandas()\n",
    "\n",
    "for i in forwardPDF['id']:\n",
    "    if i not in reversePDF['id'].values:\n",
    "        reversePDF.loc[reversePDF.shape[0]]= [i, [], []]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c117b19-8187-4a48-8ef2-e5ad20829192",
     "showTitle": false,
     "title": ""
    },
    "id": "MtAqhkvL5dIc"
   },
   "source": [
    "## **7. Page rank**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9560eeb3-b96e-418f-b9b6-2453a11d361f",
     "showTitle": false,
     "title": ""
    },
    "id": "2pS8kgam8zWP"
   },
   "source": [
    "### **7.1 Initial Page Rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ed5870-3e32-4085-a3d8-5b066e05312d",
     "showTitle": false,
     "title": ""
    },
    "id": "3zAXpybx5dIc"
   },
   "outputs": [],
   "source": [
    "# we will create a new column in the reverseDF to store the page rank of each id\n",
    "N = len(reversePDF)\n",
    "damping_factor = 0.85\n",
    "reversePDF['PageRank'] = float(damping_factor / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a21b20-1eb8-4fb1-95e7-8a7337825617",
     "showTitle": false,
     "title": ""
    },
    "id": "fOHb2COF5dId"
   },
   "source": [
    "### **7.2 Updating the page rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e55f441-ea89-4273-b5b5-828cd68aa3c4",
     "showTitle": false,
     "title": ""
    },
    "id": "WOrMtmMo5dId"
   },
   "outputs": [],
   "source": [
    "# We create broadcast variables that can be used everywhere, including functions\n",
    "d = sc.broadcast(damping_factor)\n",
    "broadcast_count_total = sc.broadcast(N)\n",
    "broadcast_count_linksPDF = sc.broadcast(OutgoingsLinksCountersPDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37739da-1e94-442e-a441-c02cf75cf305",
     "showTitle": false,
     "title": ""
    },
    "id": "7d45k7JT5dId"
   },
   "outputs": [],
   "source": [
    "def convergence(document_id, prev_PageRank, new_PageRank, threshold):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        - id_: id of the document\n",
    "        - prev_PageRank: PageRank value of the doc, of the previous iteration\n",
    "        - new_PageRank: PageRank value of the doc, of the current iteration\n",
    "        - Threshold: convergence threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    previous_value = prev_PageRank.loc[prev_PageRank['id'] == document_id, 'PageRank'].values[0]\n",
    "    new_value = new_PageRank.loc[new_PageRank['id'] == document_id, 'PageRank'].values[0]\n",
    "    \n",
    "    return 1 if (previous_value - new_value) < threshold else 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbba6b85-29bf-41e4-b049-673e1dbd75a3",
     "showTitle": false,
     "title": ""
    },
    "id": "52fNBDQw5dIe"
   },
   "outputs": [],
   "source": [
    "condition = False\n",
    "threshold = 0.00001\n",
    "\n",
    "iterations = 20     \n",
    "        \n",
    "reverseCompleteDF = sqlContext.createDataFrame(reversePDF)\n",
    "reverseCompletePDF = reverseCompleteDF.toPandas()\n",
    "reverseCompleteDF_RDD = reverseCompleteDF.rdd\n",
    "NewPageRankDF = reverseCompleteDF.select(\"id\", 'links', 'OutgoingsLinksCountersPDF', \"PageRank\")\n",
    "\n",
    "# We will exit the for loop if the PageRank has convergenced or if the number of iterations has reached the variable iterations\n",
    "for i in range(iterations):\n",
    "    # we need to calculate the share, that is used. \n",
    "    # If the document has no output links, we redistribute its rank equally among the other pages in the graph\n",
    "    share = 0\n",
    "    N = broadcast_count_total.value\n",
    "    \n",
    "    for idx in range(len(reverseCompletePDF)):\n",
    "        id_link = reverseCompletePDF.loc[idx, 'id']\n",
    "        num_links = broadcast_count_linksPDF.value\n",
    "\n",
    "        if (num_links.loc[num_links['id'] == id_link, 'count_output_links'].values[0]) == 0: #If document is a dangling node\n",
    "            page_rank = reverseCompletePDF.loc[idx, 'PageRank']\n",
    "            share = page_rank/N \n",
    "    \n",
    "    NewPageRankPDF = NewPageRankDF.toPandas()\n",
    "    for idx in range(len(reverseCompletePDF)): # this for loop updates the PageRank value of every document by using the formula\n",
    "        num_links = reverseCompletePDF.loc[idx, 'OutgoingsLinksCountersPDF']\n",
    "        list_of_ids = reverseCompletePDF.loc[idx, 'links']\n",
    "        \n",
    "        new_rank = share #new_rank will have the initial value of the share, the values of the nodes that do not have output links\n",
    "        if len(list_of_ids) != 0:\n",
    "            for l in range(len(list_of_ids)):\n",
    "                new_rank += NewPageRankPDF.loc[NewPageRankPDF['id'] == list_of_ids[l], 'PageRank'].values[0]/num_links[l]\n",
    "\n",
    "        new_page_rank = (1-d.value)/N + d.value*new_rank #We apply a damping factor to avoid closed loops problems\n",
    "        NewPageRankPDF.loc[idx, 'PageRank'] =  float(new_page_rank)\n",
    "\n",
    "    NewPageRankDF = sqlContext.createDataFrame(NewPageRankPDF)\n",
    "    #to check the condition we have created a new function (instead of using a for that would have been less efficient)\n",
    "    \n",
    "    # We create a user defined function to check whether the page rank values have converged or not\n",
    "    udf_check_condition =  udf(lambda l: convergence(l, reverseCompletePDF, NewPageRankPDF, threshold), FloatType())\n",
    "    Threshold_checkDF = reverseCompleteDF.select(\"id\", udf_check_condition(\"id\").alias(\"Condition\")) #Computing convergence\n",
    "    Threshold_checkPDF = Threshold_checkDF.toPandas()\n",
    "    \n",
    "    reverseCompleteDF_RDD = NewPageRankDF.rdd\n",
    "    reverseCompletePDF = NewPageRankPDF\n",
    "    \n",
    "    N = broadcast_count_total.value\n",
    "    iterations += 1\n",
    "                                                                         \n",
    "    # In this if statement, the two exit conditions are checked\n",
    "    if Threshold_checkPDF['Condition'].sum() == N: #Checking exit condition\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9909eb-317d-403e-97da-240d50d40fe0",
     "showTitle": false,
     "title": ""
    },
    "id": "QQUCucWnEChW"
   },
   "source": [
    "### **Validations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20cca2b2-047c-4d6a-ae5a-0218de99bad8",
     "showTitle": false,
     "title": ""
    },
    "id": "vF9qg4wO5dIe",
    "outputId": "14f51d6c-ccaf-4f2f-b4d7-03a1ee34bdd9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: 3"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " # In order to make sure that the results are correct, we check the number of unique\n",
    " # Page Rank values, and observe that indeed are a lot.\n",
    "len(NewPageRankPDF['PageRank'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00913e7f-aea7-4781-b278-017de2547163",
     "showTitle": false,
     "title": ""
    },
    "id": "dJjb5ecm5dIf",
    "outputId": "9135f461-70ae-483b-e21c-42522798a9d9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n|PageRank             |\n+---------------------+\n|2.6080153003564294E-4|\n|4.824828305659394E-4 |\n|0.001728565684882239 |\n+---------------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In addition, some final values are showed. \n",
    "NewPageRankDF.select('PageRank').distinct().show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e924a62-a555-46b8-9dba-fa63b880e111",
     "showTitle": false,
     "title": ""
    },
    "id": "hA_cfqqLFU0d"
   },
   "source": [
    "## **8. Conclusions**\n",
    "\n",
    "Throughout this final practical work, had to compute the page rank for Wikipedia's webpages using all that we have learnt during the course about Spark. In order to achieve this, we followed some previous steps:\n",
    "\n",
    "1. For each webpage, we extracted the outgoing links mentioned within the text. To do this, we made use of regular expressions because these links are always found inside \"[]\".\n",
    "\n",
    "2. We then created a dataframe containing the essential information: id, title and outgoing links.\n",
    "\n",
    "3. The next step was to convert the extracted links into ids (the extracted links could be read as titles and we wanted in them as ids.)\n",
    "\n",
    "4. We also included a column with the number of ourgoing links for each document and created a new DF for input links for each document.\n",
    "\n",
    "5. Finally, we implemented the Page Rank algorithm.\n",
    "\n",
    "The most valuable aspect of this project was gaining insight into the challenges of managing and processing large amounts of data. We made efforts to optimize resources, such as caching data structures like Dataframes, to improve computational efficiency and reduce the time required for the code to run. However, we discovered that certain operations such as toPandas() are necessary but can be costly in terms of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3faa613-aa13-4910-877f-5f0e6fde3757",
     "showTitle": false,
     "title": ""
    },
    "id": "XTlwdN92R2LN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PageRank",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
